{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "app.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "7jn5qvCJa2ia"
      },
      "outputs": [],
      "source": [
        "# pip install spacy-langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install streamlit"
      ],
      "metadata": {
        "id": "YyL9iIGYj9bx"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy_langdetect import LanguageDetector\n",
        "import unicodedata\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import sklearn\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import load_model\n",
        "\n",
        "#load stop words\n",
        "file1 = open(\"/content/list.txt\")\n",
        " \n",
        "# Use this to read file content as a stream:\n",
        "line = file1.read()\n",
        "stop_words = line.split()\n",
        "\n",
        "#Text Cleaning Final\n",
        "def remove_punctuation(text):\n",
        "  return re.sub(r'[^\\w\\s]+','',text)\n",
        "\n",
        "def remove_english_and_numbers(text):\n",
        "  return re.sub(r'[a-zA-Z\\d+]+','',text)\n",
        "\n",
        "def remove_escape_characters(text):\n",
        "  text=re.sub(r'\\n', '',text)\n",
        "  text=re.sub(r'\\_','',text)\n",
        "  return text\n",
        "\n",
        "def remove_non_arabic_non_ascii(text):\n",
        "  nlp = spacy.load('en')  # 1\n",
        "  nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
        "  text_content = text\n",
        "  doc = nlp(text_content) #3\n",
        "  detect_language = doc._.language #4\n",
        "  if detect_language['language']=='ar':\n",
        "    pass\n",
        "  else:\n",
        "      text=unicodedata.normalize('NFKD',text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "      text=remove_punctuation(text)\n",
        "      text=remove_english_and_numbers(text)\n",
        "  return text\n",
        "\n",
        "def text2words(text):\n",
        "   return text.split()\n",
        "\n",
        "def remove_stopwords(words, stop_words):\n",
        "  return [word for word in words if word not in stop_words]\n",
        "\n",
        "def normalize_text(text):\n",
        "    text=remove_punctuation(text)\n",
        "    text=remove_english_and_numbers(text)\n",
        "    text=remove_escape_characters(text)\n",
        "    text=remove_non_arabic_non_ascii(text)\n",
        "    words=text2words(text)\n",
        "    words = remove_stopwords(words, stop_words)\n",
        "    text=' '.join(words)\n",
        "    return text\n",
        "\n",
        "def detect_dialect(my_pred):\n",
        "  if my_pred=='IQ':\n",
        "    pred='اللهجة العراقية'\n",
        "  elif  my_pred=='LY':\n",
        "    pred='االلهجة الليبية'\n",
        "  elif  my_pred=='QA':\n",
        "    pred='اللهجة القطرية'\n",
        "  elif  my_pred=='PL':\n",
        "    pred='اللهجة الفلسطينية'\n",
        "  elif  my_pred=='SY':\n",
        "    pred='اللهجة السورية'\n",
        "  elif  my_pred=='TN':\n",
        "    pred='اللهجة التونسية'  \n",
        "  elif  my_pred=='JO':\n",
        "    pred='اللهجة الاردنية'\n",
        "  elif  my_pred=='MA':\n",
        "    pred='اللهجة المغربية'\n",
        "  elif  my_pred=='SA':\n",
        "    pred='اللهجة السعودية'\n",
        "  elif  my_pred=='SA':\n",
        "    pred='اللهجة السعودية'\n",
        "  elif  my_pred=='YE':\n",
        "    pred='اللهجة اليمنية'\n",
        "  elif  my_pred=='DZ':\n",
        "    pred='اللهجة الجزائرية'\n",
        "  elif  my_pred=='EG':\n",
        "    pred='اللهجة المصرية'\n",
        "  elif  my_pred=='LB':\n",
        "    pred='اللهجة اللبنانية'  \n",
        "  elif  my_pred=='KW':\n",
        "    pred='اللهجة الكويتية' \n",
        "  elif  my_pred=='OM':\n",
        "    pred='اللهجة العمانية' \n",
        "  elif  my_pred=='SD':\n",
        "    pred='اللهجة السودانية'\n",
        "  elif  my_pred=='AE':\n",
        "    pred='اللهجة الاماراتية'\n",
        "  else:\n",
        "    pred='اللهجة البحرينية'\n",
        "  return pred\n",
        "\n",
        "# loading the dataset\n",
        "data = pd.read_csv(\"/content/cleaned_539.csv\")\n",
        "y = data[\"dialect\"]\n",
        "target_names =list(y.unique())\n",
        "\n",
        "# label encoding\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "#loading the model and cv\n",
        "nb_model = pickle.load(open(\"/content/nb_model.pkl\", \"rb\"))\n",
        "cv = pickle.load(open(\"/content/transform.pkl\", \"rb\"))\n",
        "dl_model=tf.keras.models.load_model('/content/dl_model.h5',compile=False)\n",
        "with open('/content/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer= pickle.load(handle)\n",
        "\n",
        "def main():\n",
        "    selected_box = st.sidebar.selectbox(\n",
        "    'اختر من الاتي',\n",
        "    ('مرحبا','تحديد اللهجة')\n",
        "    )\n",
        "    \n",
        "    if selected_box == 'مرحبا':\n",
        "        welcome() \n",
        "    if selected_box == 'تحديد اللهجة':\n",
        "        dialect()\n",
        "\n",
        "def welcome():\n",
        "    \n",
        "    st.title('اهلا بك في برنامج تحديد اللهجات العربية')\n",
        "    st.image('/content/download.jpg',use_column_width=True)\n",
        "\n",
        "def dialect():\n",
        "  st.subheader(\"برجاء ادخال الكلام باللغة العربية لتحديد اللهجة\")\n",
        "  text = st.text_input('') #text is stored in this variable\n",
        "  text = normalize_text(text)\n",
        "  dat = [text]\n",
        "  option = st.selectbox('برجاء اختيار الموديل',\n",
        "      ('Machine learning','Deep learning'))\n",
        "  if option=='Machine learning':\n",
        "    # creating the vector\n",
        "    vect = cv.transform(dat).toarray()\n",
        "    # prediction\n",
        "    my_pred = nb_model.predict(vect)\n",
        "    my_pred = le.inverse_transform(my_pred)\n",
        "    my_pred=my_pred[0]\n",
        "  else:\n",
        "    text= tokenizer.texts_to_matrix(text,mode='tfidf')\n",
        "    # transform data\n",
        "    pred=dl_model.predict(text)\n",
        "    pred=np.argmax(pred,axis=1)\n",
        "    my_pred = le.inverse_transform(pred)\n",
        "    my_pred=my_pred[0]   \n",
        "  if st.button('توقع'):\n",
        "      my_pred=detect_dialect(my_pred)\n",
        "      st.text(my_pred)\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZneZ-ppzj7LF",
        "outputId": "0e9d74ac-ba97-45d6-a1be-3d909d4cd3cd"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "z2rLof8srRwm"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DlMz5Jguumix"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}